{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022.04.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # 모듈 내 활성화함수, 손실함수, 함수적 코딩을 위한 신경망 함수 등 존재\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)  # in-channel, out-channel, kernel-size, strid\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64)  # 4608 =  12 x 12 x 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)  # 2nd arg: kernel size\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define training and inference routines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.train()과 model.eval()을 각각 train, test(validation) 전에 선언해주는 이유는, 학습시와 추론시에 다르게 동작해야 하는 레이어가 적절히 작동하도록 하기 위함이다. 대표적으로 'Dropout', 'Batchnormalization' 레이어는 학습, 추론시에 다르게 동작한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # gradients를 backward 할 때 이전값에 더해지도록 기본 세팅되어있기 때문에\n",
    "        # 0으로 초기화 (RNN과 같은 경우에선 안해주면 됨)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y)  # nll -> negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # b_i*len(X): 누적 학습데이터포인트 수\n",
    "        # len(train_dataloader.datasets): 전체 데이터 포인트 수\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # 모든 배치에서 발생한 loss 누적시킴\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()  # view_as: 인자와 같은 size(shape)로 반환 \n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader),\n",
    "        100. * success / len(test_dataloader.dataset)\n",
    "    ))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 추가) tensor.armax()에서 keepdim의 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 2, 1]),\n",
       " tensor([[2],\n",
       "         [2],\n",
       "         [1]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],\n",
    "                  [4,5,6],\n",
    "                  [4,6,3]])\n",
    "\n",
    "# 차원이 유지 or x\n",
    "x.argmax(dim=1), x.argmax(dim=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "# Normalize의 인자는 mean, std로 각 채널에 대한 값을 튜플로 전달함\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1302,), (0.3069,))])),\n",
    "                    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1302,), (0.3069,))])),\n",
    "                    batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define optimizer and run training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.316396\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 2.074213\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.602850\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 0.883082\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.920290\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.686777\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.766417\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.339704\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.437659\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.333681\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.402200\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.262630\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.374559\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.135826\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.189522\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.215404\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.173413\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.232471\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.129612\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.074074\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.333370\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.372184\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.226448\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.359686\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.188999\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.183851\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.122123\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.339215\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.029210\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.049944\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.383828\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.118970\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.255035\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.177376\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.131559\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.065438\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.221490\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.125296\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.361108\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.069728\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.123630\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.048982\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.043404\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.041039\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.304844\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.319540\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.232836\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.299627\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.036275\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.183242\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.176690\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.198096\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.595097\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.302121\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.278291\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.094483\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.033001\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.048881\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.037928\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.250653\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.310779\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.036768\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.046197\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.076579\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.147511\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.492895\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.064178\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.027312\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.095531\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.011508\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.066745\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.070662\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.148044\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.125981\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.297581\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.215297\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.036548\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.030919\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.025826\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.115077\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.031262\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.018062\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.027123\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.020562\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.043050\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.061775\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.042444\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.051023\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.021439\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.032209\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.132035\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.043086\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.134169\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.022441\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.183527\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.042461\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.188168\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.016409\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.239811\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.092724\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.299003\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.315074\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.097258\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.081634\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.177570\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.115033\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.199133\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.135238\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.019705\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.020622\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.108559\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.028037\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.087829\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.063975\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.036795\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.018362\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.011739\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.023737\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.242777\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.145555\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.210242\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.070580\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.166146\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.151841\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.016159\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.178844\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.033273\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.196459\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.068372\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.018886\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.242015\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.057734\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.140314\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.036399\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.075168\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.267086\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.024878\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.030397\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.043575\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.030593\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.022403\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.079347\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.041066\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.099490\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.096231\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.019682\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.049317\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.121616\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.070382\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.146150\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.239053\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.071998\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.015689\n",
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.083158\n",
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.018630\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.164871\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.023489\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.120083\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.128963\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.133866\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.114442\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.069055\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.096765\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.011965\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.012287\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.046445\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.082116\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.215783\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.211373\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.041377\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.189467\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.137579\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.356393\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.031512\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.004027\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.153713\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.068074\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.183894\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.012803\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.007438\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.208785\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.070027\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.007888\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.311646\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.258563\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.066124\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.152096\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.017129\n",
      "\n",
      "Test dataset: Overall Loss: 0.0551, Overall Accuracy 9805/20 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.056741\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.089110\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.033332\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.079912\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.038775\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.004919\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.160225\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.123967\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.037578\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.014545\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.020959\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.006504\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.078369\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.025817\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.103319\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.005881\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.005003\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.084302\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.133136\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.594037\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.011413\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.035530\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.011587\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.012055\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.009610\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.050046\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.008267\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.171038\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.002984\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.011145\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.140753\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.004964\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.125378\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.008049\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.003924\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.005594\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.010514\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.008117\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.068124\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.003721\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.161583\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.222718\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.015832\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.076256\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.004973\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.207741\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.037218\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.078503\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.204702\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.051811\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.220697\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.044232\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.261369\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.099818\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.005280\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.017072\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.102173\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.048195\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.039142\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.060081\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.340863\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.032134\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.045998\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.004753\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.040104\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.019256\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.020189\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.123482\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.027882\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.063899\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.016649\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.003809\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.030443\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.044369\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.027872\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.041280\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.030224\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.141590\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.047710\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.029575\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.005009\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.007940\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.003955\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.015850\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.035108\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.000422\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.029574\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 0.080365\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 0.045503\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.003780\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.019565\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.015288\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 0.052656\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.000645\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.007602\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.044676\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.106860\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.098700\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.005643\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.032263\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.028857\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.002690\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.050171\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.077605\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.049674\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.020772\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.346126\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.079308\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.038161\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.029625\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.045361\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.038391\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.005807\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.102961\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.093223\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.090252\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.001867\n",
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.032627\n",
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.003154\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.093292\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.020926\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.054254\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.010872\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.006858\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.022601\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.016846\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.059762\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.104440\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.257518\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.015824\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.013651\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.101558\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.036339\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.031639\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.052435\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.053680\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.017260\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.206545\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.005483\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.053785\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.063792\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.008632\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.022884\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.224141\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.049011\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.052908\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.129829\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.007166\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.009783\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.280920\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.003068\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.020515\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.062493\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.006672\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.051154\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.089955\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.005774\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.012697\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.003604\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.028354\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.007181\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.003891\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.042276\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.048582\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.144326\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.020104\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.207796\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.044338\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.100030\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.011672\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.061410\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.056338\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.006075\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.040758\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.055970\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.028773\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.005990\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.300367\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.215377\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.005276\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.350242\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.046414\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.090179\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.056380\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.007959\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.012087\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.004568\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.051754\n",
      "\n",
      "Test dataset: Overall Loss: 0.0417, Overall Accuracy 9855/20 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run inference on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "sample_data\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')  # 배치의 첫번째 데이터\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([500, 1, 28, 28]), torch.Size([1, 28, 28]), torch.Size([28, 28]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.size(), sample_data[0].size(), sample_data[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1140e+01, -2.2123e+01, -1.7181e+01, -1.5219e+01, -2.1893e+01,\n",
       "         -2.3477e+01, -2.8700e+01, -3.5763e-07, -2.1654e+01, -1.6887e+01],\n",
       "        [-1.6975e+01, -1.2004e+01, -6.1989e-06, -1.8365e+01, -2.2363e+01,\n",
       "         -2.5181e+01, -1.6664e+01, -2.2825e+01, -1.9987e+01, -2.6786e+01],\n",
       "        [-1.6359e+01, -3.0398e-05, -1.1947e+01, -1.6026e+01, -1.1100e+01,\n",
       "         -1.4184e+01, -1.4796e+01, -1.2334e+01, -1.2820e+01, -1.4475e+01],\n",
       "        [-2.2053e-05, -1.5600e+01, -1.1197e+01, -1.7885e+01, -1.5496e+01,\n",
       "         -1.7198e+01, -1.2067e+01, -1.5905e+01, -1.4320e+01, -1.3423e+01],\n",
       "        [-2.0746e+01, -1.9006e+01, -1.9481e+01, -2.4923e+01, -1.6808e-05,\n",
       "         -2.0860e+01, -2.1599e+01, -1.7445e+01, -1.7957e+01, -1.0992e+01]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델에 테스트 데이터를 입력한 출력: 배치의 데이터포인트 수 x 클래스별 확률\n",
    "print(model(sample_data).data.size())\n",
    "model(sample_data).data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([-3.5763e-07, -6.1989e-06, -3.0398e-05, -2.2053e-05, -1.6808e-05,\n",
       "        -2.0861e-05, -4.8636e-05, -7.5407e-04, -1.0305e-02, -4.4467e-04,\n",
       "        -3.6955e-06, -2.0266e-06, -1.1670e-04, -8.3446e-07, -5.3644e-06,\n",
       "        -5.2808e-05, -2.9802e-05, -2.1458e-06, -1.1653e-02, -2.3842e-07,\n",
       "        -1.0734e-03, -1.5329e-04, -4.1723e-06, -4.7684e-07, -7.2357e-05,\n",
       "        -4.2438e-05, -1.8954e-05, -2.3842e-07, -5.2212e-05, -2.8133e-05,\n",
       "        -1.5259e-05, -4.0809e-04, -3.5763e-07, -4.1190e-04, -3.3379e-06,\n",
       "        -9.5367e-07, -2.5153e-05, -9.4175e-06, -2.8610e-06, -2.6583e-05,\n",
       "        -8.6781e-05, -3.6477e-05, -9.5720e-05, -2.4244e-04, -1.7665e-04,\n",
       "        -7.2717e-06, -1.3527e-03, -2.3842e-07, -4.3391e-05, -5.9605e-07,\n",
       "        -1.1921e-06, -5.5788e-05, -3.5763e-07, -1.2063e-04, -4.7684e-07,\n",
       "        -4.2915e-06, -1.1921e-07, -5.9245e-05, -5.2452e-06, -3.3470e-03,\n",
       "        -3.5763e-06, -1.2713e-03, -1.3970e-01, -5.3552e-02, -1.9073e-06,\n",
       "        -6.2375e-04, -1.0014e-05, -2.0861e-05, -3.5285e-05, -3.5762e-05,\n",
       "        -9.5367e-07, -3.0994e-06, -3.7073e-05, -1.3160e-02, -2.5630e-05,\n",
       "        -1.1921e-07, -1.3113e-06, -1.1206e-05, -1.9838e-03, -6.7949e-06,\n",
       "        -1.2810e-03, -1.8239e-05,  0.0000e+00, -4.7684e-07, -1.2517e-05,\n",
       "        -1.4305e-06, -5.4836e-06, -1.0354e-02, -5.9605e-07, -2.1100e-05,\n",
       "        -1.1921e-06, -5.9605e-07, -5.1217e-02, -4.2762e-03, -1.0478e-04,\n",
       "        -8.6842e-04, -3.6521e-02, -1.5281e-04, -2.0394e-03, -1.0729e-06,\n",
       "        -8.3446e-07, -4.7684e-07, -1.3113e-06,  0.0000e+00, -5.0186e-05,\n",
       "        -4.2915e-06, -2.6226e-06, -1.0406e-03, -5.2570e-05,  0.0000e+00,\n",
       "        -1.0729e-06, -2.4165e-03, -2.6226e-06, -5.2452e-06, -2.5868e-05,\n",
       "        -1.8122e-01, -6.2583e-05, -1.6689e-06, -1.1252e-03, -4.0750e-03,\n",
       "        -4.0292e-05, -5.9623e-04, -6.3181e-06, -4.7684e-07, -3.5643e-05,\n",
       "        -3.1304e-03, -1.8952e-03, -7.1526e-07, -3.5763e-06, -2.3842e-07,\n",
       "        -2.3126e-05, -9.5367e-06, -5.3285e-05, -1.0800e-04, -3.5763e-07,\n",
       "        -3.8147e-06, -2.1458e-06, -1.5139e-05, -2.3842e-07, -2.7213e-03,\n",
       "        -1.6570e-05, -1.1921e-07, -9.1356e-04, -1.2159e-05, -2.7418e-06,\n",
       "        -2.0861e-05, -7.3909e-06,  0.0000e+00, -1.5497e-06, -5.3126e-02,\n",
       "        -5.2452e-06, -8.7562e-03, -2.3842e-07, -5.9604e-06, -9.2860e-05,\n",
       "        -2.5034e-06, -3.5763e-06, -3.5763e-06, -1.3255e-02, -2.8737e-04,\n",
       "        -7.4049e-04, -5.9605e-07,  0.0000e+00, -2.3842e-06, -7.0212e-05,\n",
       "        -6.3181e-06,  0.0000e+00, -3.2646e-04, -2.1458e-06, -1.1921e-07,\n",
       "        -2.1458e-06, -3.4681e-03, -1.9073e-06, -2.4199e-05, -3.5763e-07,\n",
       "        -2.5003e-02, -4.1722e-05, -2.7179e-05, -5.0068e-06, -1.3053e-04,\n",
       "        -1.5020e-05, -1.7524e-05, -2.4795e-05, -1.4186e-05, -6.2086e-01,\n",
       "        -2.1324e-04, -1.5497e-06, -5.6028e-06, -3.5763e-07, -1.1616e-03,\n",
       "        -6.3590e-04, -1.3297e-03, -5.4836e-06, -8.9176e-04, -6.9141e-06,\n",
       "        -6.4664e-03, -4.0054e-05, -7.0333e-06, -1.7881e-06, -3.6506e-03,\n",
       "        -1.4437e-03, -7.1526e-07, -2.3126e-05, -1.2921e-04, -3.2424e-05,\n",
       "        -1.5497e-06, -8.2728e-05, -2.6464e-05, -4.7684e-07, -1.0025e-02,\n",
       "        -5.9605e-07, -2.3621e-03, -3.4571e-06, -6.9258e-05, -4.6791e-04,\n",
       "        -1.4674e-04, -2.8610e-06, -1.1178e-02, -8.3446e-07, -1.9073e-06,\n",
       "        -2.2814e-04, -1.9131e-04, -7.9870e-06, -3.4571e-06, -2.0137e-03,\n",
       "        -1.0014e-05, -5.7815e-05, -7.8675e-05, -1.0133e-05, -3.9457e-05,\n",
       "        -6.7200e-04, -1.2875e-05, -3.5793e-03, -1.5891e-03, -2.5272e-05,\n",
       "        -2.3722e-05, -9.5367e-07, -3.6955e-06, -1.1921e-07, -9.2983e-06,\n",
       "        -4.2676e-05, -8.2609e-05, -5.8264e-04, -5.7815e-05, -2.0447e-03,\n",
       "        -1.3801e-03, -4.6492e-06, -7.9972e-01, -4.0531e-06, -3.1232e-05,\n",
       "        -1.7426e-03, -2.4519e-03, -1.0014e-05, -1.7881e-06, -9.6559e-06,\n",
       "        -1.9370e-04, -6.0797e-06, -7.8016e-04, -4.7684e-07, -6.2764e-01,\n",
       "        -2.3842e-07, -2.5034e-06,  0.0000e+00, -2.3842e-07, -1.1053e-02,\n",
       "        -7.7486e-06, -4.2412e-01, -9.1791e-06, -1.8013e-02, -4.2199e-05,\n",
       "        -5.0068e-06, -2.0385e-05, -6.9141e-06, -5.1855e-05, -1.7047e-05,\n",
       "        -1.0371e-05, -1.1921e-06, -3.3826e-04, -2.8610e-06, -1.8596e-05,\n",
       "        -1.3232e-05, -5.8412e-06, -1.1233e-02, -4.7684e-07, -2.1338e-05,\n",
       "        -1.1921e-07, -1.9167e-04, -8.8449e-05, -2.4080e-05, -6.0743e-04,\n",
       "        -1.3743e-01, -3.0636e-05, -6.4352e-04, -1.0371e-05, -5.9605e-07,\n",
       "        -2.9802e-06, -2.8729e-05, -4.5897e-04, -8.3446e-07, -2.8610e-06,\n",
       "        -1.5909e-03, -3.6716e-05, -2.2650e-06, -9.2988e-04,  0.0000e+00,\n",
       "        -4.7112e-04, -4.7684e-07, -4.1007e-05, -1.9073e-06, -1.6689e-06,\n",
       "        -1.9073e-06, -4.5775e-05, -7.6294e-06, -2.0943e-04, -7.9870e-06,\n",
       "        -8.5830e-06, -8.1062e-06, -7.0212e-05, -5.4583e-04, -3.5763e-06,\n",
       "        -6.8359e-01, -8.7477e-03, -1.8368e-04, -2.3842e-07, -1.2206e-03,\n",
       "        -2.7895e-05, -1.2145e-03, -1.4543e-05, -3.3379e-06, -5.9604e-06,\n",
       "        -1.1563e-05, -3.5763e-07, -4.0531e-06, -4.0292e-05, -1.1921e-07,\n",
       "        -2.3842e-06, -8.3936e-04, -3.6352e-04, -1.9429e-03, -5.1759e-04,\n",
       "        -1.7620e-01, -3.3221e-03, -8.1062e-06, -7.3909e-06, -1.8094e-04,\n",
       "        -2.7537e-05, -7.4622e-05, -7.7486e-06, -2.6226e-06, -2.2428e-03,\n",
       "        -4.1440e-04, -1.5497e-06, -9.1072e-05, -4.5539e-04, -1.1265e-04,\n",
       "        -9.2145e-05, -4.7684e-07, -2.3906e-03, -1.4585e-03, -1.3287e-01,\n",
       "        -4.2915e-06, -5.9605e-07, -1.8852e-03, -8.0701e-05, -5.1260e-06,\n",
       "        -1.4067e-05, -3.3561e-03, -1.6689e-06, -7.5395e-04, -1.5139e-05,\n",
       "        -2.1458e-06, -1.3113e-06, -7.4503e-05, -6.7949e-06, -3.6120e-05,\n",
       "        -1.1921e-07, -2.2230e-04, -1.2040e-05, -2.8610e-06, -1.5497e-06,\n",
       "        -3.5405e-05, -7.6967e-02, -1.1921e-05, -2.9087e-05,  0.0000e+00,\n",
       "        -5.6028e-06, -2.5525e-03, -1.1206e-05, -2.4557e-05, -4.0475e-04,\n",
       "        -5.3644e-06, -1.1420e-04, -4.7684e-07, -3.3855e-05, -2.0178e-02,\n",
       "        -8.8214e-06, -3.5763e-07, -1.3553e-04, -4.7684e-07, -7.5102e-06,\n",
       "        -2.9667e-04, -8.9407e-06, -1.5497e-06, -2.1134e-03, -2.1767e-03,\n",
       "        -2.0097e-04, -5.2591e-03, -9.6559e-06, -1.4663e-05, -5.0770e-04,\n",
       "         0.0000e+00, -9.1261e-04, -1.4795e-02, -1.3947e-05, -1.1694e-04,\n",
       "        -1.8120e-05, -3.2186e-06, -2.6998e-03,  0.0000e+00, -1.2755e-05,\n",
       "        -3.9384e-03, -5.7585e-04, -5.9722e-05, -1.4305e-06, -2.9802e-06,\n",
       "        -2.2650e-06, -1.4067e-05, -1.8716e-05, -1.5497e-06, -7.1526e-07,\n",
       "        -4.2915e-06, -2.1524e-03, -1.1551e-04, -4.4107e-06, -2.4318e-05,\n",
       "        -8.0615e-01, -9.0476e-05, -7.9870e-06, -2.6985e-04, -5.1378e-05,\n",
       "        -4.7684e-06, -1.2994e-05, -7.0333e-06, -2.6898e-03, -3.8756e-03,\n",
       "        -1.8377e-01, -2.1920e-04, -4.3258e-01, -9.0463e-04, -5.1529e-01,\n",
       "        -1.0868e-02, -1.2206e-04, -2.3541e-04, -8.3446e-07, -1.7881e-06,\n",
       "        -2.4318e-05, -6.4674e-04, -1.8158e-03,  0.0000e+00, -4.2080e-05,\n",
       "        -1.0827e-03, -1.0729e-06, -1.0727e-03, -1.4305e-06, -3.8486e-02,\n",
       "        -3.5316e-03, -2.3842e-06, -1.1086e-05, -1.6798e-03, -6.6151e-04,\n",
       "        -1.3267e-04, -5.2152e-04, -9.5367e-07, -3.3259e-05, -2.3246e-05,\n",
       "        -2.3007e-05, -2.9802e-06, -5.6028e-06, -7.6849e-04, -1.7601e-03,\n",
       "        -6.2345e-05, -6.0797e-06, -1.5497e-06, -1.5675e-04, -9.5367e-07,\n",
       "        -4.6371e-05, -1.7546e-04, -1.6020e-04, -1.2647e-04, -2.3007e-05,\n",
       "        -1.9439e-02, -4.4991e-04, -2.4411e-04,  0.0000e+00, -3.5763e-07,\n",
       "        -4.6839e-01, -7.0333e-06, -1.8448e-03, -1.0764e-04, -1.6998e-04]),\n",
       "indices=tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
       "        4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
       "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4, 6, 4, 3, 0, 7, 0,\n",
       "        2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 9, 3, 1, 4,\n",
       "        1, 7, 6, 9, 6, 0, 5, 4, 9, 9, 2, 1, 9, 4, 8, 7, 3, 9, 7, 9, 4, 4, 9, 2,\n",
       "        5, 4, 7, 6, 7, 9, 0, 5, 8, 5, 6, 6, 5, 7, 8, 1, 0, 1, 6, 4, 6, 7, 3, 1,\n",
       "        7, 1, 8, 2, 0, 2, 9, 9, 5, 5, 1, 5, 6, 0, 3, 4, 4, 6, 5, 4, 6, 5, 4, 5,\n",
       "        1, 4, 4, 7, 2, 3, 2, 7, 1, 8, 1, 8, 1, 8, 5, 0, 8, 9, 2, 5, 0, 1, 1, 1,\n",
       "        0, 9, 0, 3, 1, 6, 4, 2, 3, 6, 1, 1, 1, 3, 9, 5, 2, 9, 4, 5, 9, 3, 9, 0,\n",
       "        3, 6, 5, 5, 7, 2, 2, 7, 1, 2, 8, 4, 1, 7, 3, 3, 8, 8, 7, 9, 2, 2, 4, 1,\n",
       "        5, 9, 8, 7, 2, 3, 0, 4, 4, 2, 4, 1, 9, 5, 7, 7, 2, 8, 2, 6, 8, 5, 7, 7,\n",
       "        9, 1, 8, 1, 8, 0, 3, 0, 1, 9, 9, 4, 1, 8, 2, 1, 2, 9, 7, 5, 9, 2, 6, 4,\n",
       "        1, 5, 8, 2, 9, 2, 0, 4, 0, 0, 2, 8, 4, 7, 1, 2, 4, 0, 2, 7, 4, 3, 3, 0,\n",
       "        0, 3, 1, 9, 6, 5, 2, 5, 8, 7, 9, 3, 0, 4, 2, 0, 7, 1, 1, 2, 1, 5, 3, 3,\n",
       "        9, 7, 8, 6, 5, 6, 1, 3, 8, 1, 0, 5, 1, 3, 1, 5, 5, 6, 1, 8, 5, 1, 7, 9,\n",
       "        4, 6, 2, 2, 5, 0, 6, 5, 6, 3, 7, 2, 0, 8, 8, 5, 4, 1, 1, 4, 0, 3, 3, 7,\n",
       "        6, 1, 6, 2, 1, 9, 2, 8, 6, 1, 9, 5, 2, 5, 4, 4, 2, 8, 3, 8, 2, 4, 5, 0,\n",
       "        3, 1, 7, 7, 5, 7, 9, 7, 1, 9, 2, 1, 4, 2, 9, 2, 0, 4, 9, 1, 4, 8, 1, 8,\n",
       "        4, 5, 9, 8, 8, 3, 7, 6, 0, 0, 3, 0, 2, 0, 6, 4, 9, 5, 3, 3, 2, 3, 9, 1,\n",
       "        2, 6, 8, 0, 5, 6, 6, 6, 3, 8, 8, 2, 7, 5, 8, 9, 6, 1, 8, 4, 1, 2, 5, 9,\n",
       "        1, 9, 7, 5, 4, 0, 8, 9, 9, 1, 0, 5, 2, 3, 7, 8, 9, 4, 0, 6]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy에서 max와 argmax를 동시에 수행해준다고 보면 될듯\n",
    "model(sample_data).data.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample_data).data.max(1)[1][0]  # 1로 라벨값에 접근하여 첫번째 값을 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 7\n",
      "Ground truth is : 7\n"
     ]
    }
   ],
   "source": [
    "print(f'Model prediction is : {model(sample_data).data.max(1)[1][0]}')\n",
    "print(f'Ground truth is : {sample_targets[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mmseg torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e074707929e4ad7a086f0f500cacedcbe9b76ff0ab2b02961d070a1c30eb00f2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('neo37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
